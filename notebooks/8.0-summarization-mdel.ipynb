{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf3d97ae-2e10-4af6-a8de-aa1280d75586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parameters for flan-t5 family: small 80M, base 250M, large 780M, xl 3B, xxl 11B\n",
    "model_id = google/flan-t5-small\n",
    "\n",
    "# https://huggingface.co/datasets/billsum\n",
    "dataset_name, dataset_version = cnn_dailymail, 3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d6def1c-1bdb-4a8d-a584-c4a735a95352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.219.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e6e237-dbe9-4205-978d-2e28da68a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:training set: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 287113\n",
      "})\n",
      "INFO:__main__:validation set: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 11490\n",
      "})\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  0%|                                    | 2/430671 [00:05<345:03:49,  2.88s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model-name t5-small --train-dir local_train_data --valid-dir local_valid_data --output-data-dir ./output --model-dir ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376ad351-f33e-4240-961b-014a524cb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sagemaker-user/styx_notebooks/notebooks/train2.py\", line 89, in <module>\n",
      "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name, quantization_config=bnb_config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3202, in from_pretrained\n",
      "    hf_quantizer.validate_environment(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_8bit.py\", line 62, in validate_environment\n",
      "    raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n",
      "RuntimeError: No GPU found. A GPU is needed for quantization.\n"
     ]
    }
   ],
   "source": [
    "!python train2.py --model-name t5-small --train-dir local_train_data --valid-dir local_valid_data --output-data-dir ./output --model-dir ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a4740-cc92-45cd-8a90-1d23289e9071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cc0b7-3523-4401-a76b-71f60182c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb00797-9850-4a41-9858-c63504630ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302d200-300d-4efd-9446-b8263bd387cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_dataset(dataset_name, dataset_version)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff9104-1c14-4b4f-be11-f8f8f01996e8",
   "metadata": {},
   "source": [
    "Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a875f9a-eae7-429c-8b8b-5e348ff3da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "prefix = summarize: \n",
    "input_max_length = 1024\n",
    "output_max_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[article]]\n",
    "    model_inputs = tokenizer(inputs, max_length=input_max_length, truncation=True)\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[highlights], max_length=output_max_length, truncation=True\n",
    "    )\n",
    "    model_inputs[labels] = labels[input_ids]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23e45d-d5c5-4fa6-a7f7-1590068f06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=[article, highlights, id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaded14-9a6b-45fd-ad22-05f61614dc94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "524521ba-5060-4be5-a784-12d3dde096fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://styx-nlp-datasets/huggingface/cnn_dailymail-t5-summarization\n",
      "s3://styx-nlp-datasets/huggingface/cnn_dailymail-t5-summarization/train\n",
      "s3://styx-nlp-datasets/huggingface/cnn_dailymail-t5-summarization/validation\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = styx-nlp-datasets  # Replace with your actual bucket name\n",
    "s3_prefix = huggingface/cnn_dailymail-t5-summarization\n",
    "\n",
    "dataset_input_path = s3://{}/{}.format(bucket, s3_prefix)\n",
    "train_input_path = {}/train.format(dataset_input_path)\n",
    "valid_input_path = {}/validation.format(dataset_input_path)\n",
    "\n",
    "print(dataset_input_path)\n",
    "print(train_input_path)\n",
    "print(valid_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "552aa499-d3ac-4654-bdea-4ba7f744be50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99630d5a04b4c548100f332706d47d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9407afcdace04877b05c6d7b50f34466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/cnn_dailymail-t5-summarization/train/data-00000-of-00003.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/train/data-00001-of-00003.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/train/data-00002-of-00003.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/train/dataset_info.json\n",
      "huggingface/cnn_dailymail-t5-summarization/train/state.json\n",
      "huggingface/cnn_dailymail-t5-summarization/validation/data-00000-of-00001.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/validation/dataset_info.json\n",
      "huggingface/cnn_dailymail-t5-summarization/validation/state.json\n"
     ]
    }
   ],
   "source": [
    "# Save tokenized dataset locally first\n",
    "local_train_path = local_train_data\n",
    "local_valid_path = local_valid_data\n",
    "\n",
    "tokenized_dataset[train].save_to_disk(local_train_path)\n",
    "tokenized_dataset[test].save_to_disk(local_valid_path)\n",
    "\n",
    "# Upload the local files to S3\n",
    "for root, dirs, files in os.walk(local_train_path):\n",
    "    for file in files:\n",
    "        s3_client.upload_file(\n",
    "            os.path.join(root, file),\n",
    "            bucket,\n",
    "            os.path.join(s3_prefix, 'train', os.path.relpath(os.path.join(root, file), local_train_path))\n",
    "        )\n",
    "\n",
    "for root, dirs, files in os.walk(local_valid_path):\n",
    "    for file in files:\n",
    "        s3_client.upload_file(\n",
    "            os.path.join(root, file),\n",
    "            bucket,\n",
    "            os.path.join(s3_prefix, 'validation', os.path.relpath(os.path.join(root, file), local_valid_path))\n",
    "        )\n",
    "\n",
    "# Verify by listing the uploaded files\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=s3_prefix)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5564bbb-ce33-4b83-9082-443ed96c3581",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_disk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m(train_input_path)\n\u001b[1;32m      2\u001b[0m valid_ds \u001b[38;5;241m=\u001b[39m load_from_disk(valid_input_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_from_disk' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds = load_from_disk(train_input_path)\n",
    "valid_ds = load_from_disk(valid_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efaf2b74-7f0d-4e0d-a597-cd5d7fb3ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 23:17:11  424111416 huggingface/cnn_dailymail-t5-summarization/train/data-00000-of-00003.arrow\n",
      "2024-05-30 23:17:18  460139096 huggingface/cnn_dailymail-t5-summarization/train/data-00001-of-00003.arrow\n",
      "2024-05-30 23:17:26  453302224 huggingface/cnn_dailymail-t5-summarization/train/data-00002-of-00003.arrow\n",
      "2024-05-30 23:17:18       2195 huggingface/cnn_dailymail-t5-summarization/train/dataset_info.json\n",
      "2024-05-30 23:17:31        368 huggingface/cnn_dailymail-t5-summarization/train/state.json\n",
      "2024-05-30 23:17:32   52967328 huggingface/cnn_dailymail-t5-summarization/validation/data-00000-of-00001.arrow\n",
      "2024-05-30 23:17:33       2195 huggingface/cnn_dailymail-t5-summarization/validation/dataset_info.json\n",
      "2024-05-30 23:17:33        249 huggingface/cnn_dailymail-t5-summarization/validation/state.json\n"
     ]
    }
   ],
   "source": [
    "%%sh -s $dataset_input_path\n",
    "aws s3 ls --recursive $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a8d8027-2758-4fba-a4ce-8a5fd130ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mevaluate\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    AutoModelForSeq2SeqLM,\u001b[37m\u001b[39;49;00m\n",
      "    AutoTokenizer,\u001b[37m\u001b[39;49;00m\n",
      "    DataCollatorForSeq2Seq,\u001b[37m\u001b[39;49;00m\n",
      "    Seq2SeqTrainer,\u001b[37m\u001b[39;49;00m\n",
      "    Seq2SeqTrainingArguments,\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "rouge = evaluate.load(\u001b[33m\"\u001b[39;49;00m\u001b[33mrouge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(eval_pred):\u001b[37m\u001b[39;49;00m\n",
      "    predictions, labels = eval_pred\u001b[37m\u001b[39;49;00m\n",
      "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    labels = np.where(labels != -\u001b[34m100\u001b[39;49;00m, labels, tokenizer.pad_token_id)\u001b[37m\u001b[39;49;00m\n",
      "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) \u001b[34mfor\u001b[39;49;00m pred \u001b[35min\u001b[39;49;00m predictions]\u001b[37m\u001b[39;49;00m\n",
      "    result[\u001b[33m\"\u001b[39;49;00m\u001b[33mgen_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.mean(prediction_lens)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m {k: \u001b[36mround\u001b[39;49;00m(v, \u001b[34m4\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m result.items()}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# hyperparameters are passed as command-line arguments to the script.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m8\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--evaluation-strategy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save-strategy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mno\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save-steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--valid-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALID\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.train_dir)\u001b[37m\u001b[39;49;00m\n",
      "    valid_dataset = load_from_disk(args.valid_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtraining set: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_dataset\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation set: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalid_dataset\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# download model from model hub\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# download the tokenizer too, which will be saved in the model artifact\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# and used at prediction time\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    training_args = Seq2SeqTrainingArguments(\u001b[37m\u001b[39;49;00m\n",
      "        output_dir=args.model_dir,\u001b[37m\u001b[39;49;00m\n",
      "        num_train_epochs=args.epochs,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_train_batch_size=args.train_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        save_strategy=args.save_strategy,\u001b[37m\u001b[39;49;00m\n",
      "        save_steps=args.save_steps,\u001b[37m\u001b[39;49;00m\n",
      "        evaluation_strategy=args.evaluation_strategy,\u001b[37m\u001b[39;49;00m\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\u001b[37m\u001b[39;49;00m\n",
      "        predict_with_generate=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        fp16=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# create trainer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer = Seq2SeqTrainer(\u001b[37m\u001b[39;49;00m\n",
      "        model=model,\u001b[37m\u001b[39;49;00m\n",
      "        args=training_args,\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "        train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        eval_dataset=valid_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        data_collator=data_collator,\u001b[37m\u001b[39;49;00m\n",
      "        compute_metrics=compute_metrics,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# train model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer.train()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12c3bb4a-f5a4-4389-bbb7-7c44c4dac58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (2023.6.0)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing requirements for fsspec: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-2023.6.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: fsspec, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "\u001b[33m    WARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: fsspec 2023.6.0\n",
      "\u001b[31mERROR: Cannot uninstall fsspec 2023.6.0, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps fsspec==2023.6.0'.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade fsspec datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0d1fa59-c3a4-4199-94c6-c27cdeb98242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3109, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2902, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3100, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2902, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 45, in run\n",
      "    if not print_results(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 150, in print_results\n",
      "    for i, dist in enumerate(distributions):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 103, in search_packages_info\n",
      "    requires = sorted((req.name for req in dist.iter_dependencies()), key=str.lower)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/metadata/pkg_resources.py\", line 221, in iter_dependencies\n",
      "    return self._dist.requires(extras)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2822, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3111, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3102, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1519, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1727, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-2023.6.0.dist-info/METADATA'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3109, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2902, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3100, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2902, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 45, in run\n",
      "    if not print_results(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 150, in print_results\n",
      "    for i, dist in enumerate(distributions):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 104, in search_packages_info\n",
      "    required_by = sorted(_get_requiring_packages(dist), key=str.lower)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 94, in <genexpr>\n",
      "    in {canonicalize_name(d.name) for d in dist.iter_dependencies()}\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/metadata/pkg_resources.py\", line 221, in iter_dependencies\n",
      "    return self._dist.requires(extras)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2822, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3111, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3102, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1519, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1727, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-2023.6.0.dist-info/METADATA'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3109, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2902, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3100, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2902, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 45, in run\n",
      "    if not print_results(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 150, in print_results\n",
      "    for i, dist in enumerate(distributions):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 104, in search_packages_info\n",
      "    required_by = sorted(_get_requiring_packages(dist), key=str.lower)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/commands/show.py\", line 94, in <genexpr>\n",
      "    in {canonicalize_name(d.name) for d in dist.iter_dependencies()}\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/metadata/pkg_resources.py\", line 221, in iter_dependencies\n",
      "    return self._dist.requires(extras)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2822, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3111, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3102, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1519, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1727, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-2023.6.0.dist-info/METADATA'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip show fsspec\n",
    "!pip show datasets\n",
    "!pip show evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "526f4ea2-90be-4f25-a45f-dd347eb610a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    epochs: 1,\n",
    "    learning-rate: 1e-6,\n",
    "    train-batch-size: 1,\n",
    "    eval-batch-size: 8,\n",
    "    model-name: model_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5efa652-e366-4e6c-a985-3572d373d05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::381491949871:role/service-role/AmazonSageMaker-ExecutionRole-20240514T182453'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a100378-ab6a-44b4-9b88-14e7e7af7885",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:iam::381491949871:user/mlflow-user is not authorized to perform: iam:GetRole on resource: role AmazonSageMaker-ExecutionRole-20240514T182453 because no identity-based policy allows the iam:GetRole action",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      4\u001b[0m iam_client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m role \u001b[38;5;241m=\u001b[39m \u001b[43miam_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_role\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRoleName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAmazonSageMaker-ExecutionRole-20240514T182453\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRole\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m sess \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mSession()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:iam::381491949871:user/mlflow-user is not authorized to perform: iam:GetRole on resource: role AmazonSageMaker-ExecutionRole-20240514T182453 because no identity-based policy allows the iam:GetRole action"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName=AmazonSageMaker-ExecutionRole-20240514T182453)['Role']['Arn']\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e089c355-02a3-4f1c-b729-78652fe8028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name mlflow-user to get Role path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The current AWS identity is not a role: arn:aws:iam::381491949871:user/mlflow-user, therefore it cannot be used as a SageMaker execution role",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFace\n\u001b[1;32m      3\u001b[0m huggingface_estimator \u001b[38;5;241m=\u001b[39m HuggingFace(\n\u001b[0;32m----> 4\u001b[0m     role\u001b[38;5;241m=\u001b[39m\u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_execution_role\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Fine-tuning script\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     dependencies\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequirements.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Infrastructure\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     transformers_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.26.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     pytorch_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.13.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     py_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy39\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.p3dn.24xlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m     distribution\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmdistributed\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataparallel\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}}},\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:7360\u001b[0m, in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session, use_default)\u001b[0m\n\u001b[1;32m   7354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iam_client\u001b[38;5;241m.\u001b[39mget_role(RoleName\u001b[38;5;241m=\u001b[39mdefault_role_name)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRole\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   7356\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   7357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current AWS identity is not a role: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, therefore it cannot be used as a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   7358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSageMaker execution role\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   7359\u001b[0m )\n\u001b[0;32m-> 7360\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message\u001b[38;5;241m.\u001b[39mformat(arn))\n",
      "\u001b[0;31mValueError\u001b[0m: The current AWS identity is not a role: arn:aws:iam::381491949871:user/mlflow-user, therefore it cannot be used as a SageMaker execution role"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    # Fine-tuning script\n",
    "    entry_point=train.py,\n",
    "    dependencies=[requirements.txt],\n",
    "    hyperparameters=hyperparameters,\n",
    "    # Infrastructure\n",
    "    transformers_version=4.26.0,\n",
    "    pytorch_version=1.13.1,\n",
    "    py_version=py39,\n",
    "    instance_type=ml.p3dn.24xlarge,\n",
    "    instance_count=1,\n",
    "    distribution={smdistributed: {dataparallel: {enabled: True}}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431ba7b-4246-4182-8f22-5783d911ea59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673e433-eb23-4622-b667-1f3fccca19d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c49a4f-1937-4cdb-bb0a-a25458fd5f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94899131-413e-498e-a51e-cb112095bf6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'S3FileSystem' from 'datasets.filesystems' (/opt/conda/lib/python3.10/site-packages/datasets/filesystems/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3FileSystem\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'S3FileSystem' from 'datasets.filesystems' (/opt/conda/lib/python3.10/site-packages/datasets/filesystems/__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets.filesystems import S3FileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ab92b-f498-4e2c-94e8-13766f0b8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = huggingface/billsum-t5-summarization\n",
    "\n",
    "dataset_input_path = s3://{}/{}.format(bucket, s3_prefix)\n",
    "train_input_path = {}/train.format(dataset_input_path)\n",
    "valid_input_path = {}/validation.format(dataset_input_path)\n",
    "\n",
    "print(dataset_input_path)\n",
    "print(train_input_path)\n",
    "print(valid_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb278ef-a4bd-4291-9975-1139998ca421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9094153-2ad9-41f7-80ab-e2062783ebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe40fc6-1869-4d6b-b0e1-9d04c92db418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_string_file_s3(bucket, s3_path):\n",
    "    \n",
    "    Read a file to an S3 bucket\n",
    "    :param bucket: S3 bucket name\n",
    "    :param s3_path: S3 path where the file will be stored\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=s3_path)\n",
    "        return StringIO(obj['Body'].read().decode('utf-8'))\n",
    "    except FileNotFoundError:\n",
    "        print(The file was not found.)\n",
    "    except NoCredentialsError:\n",
    "        print(Credentials not available.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a484d37c-fe20-4b60-8d7d-2b574bb2cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = external/sentiment/Fin_Cleaned.csv\n",
    "bucket_name = styx-nlp-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9e980ed-f41b-4c7d-b11c-237ef4cd710a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NoCredentialsError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mread_string_file_s3\u001b[0;34m(bucket, s3_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     obj \u001b[38;5;241m=\u001b[39m s3\u001b[38;5;241m.\u001b[39mget_object(Bucket\u001b[38;5;241m=\u001b[39mbucket, Key\u001b[38;5;241m=\u001b[39ms3_path)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStringIO\u001b[49m(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StringIO' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mread_string_file_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mread_string_file_s3\u001b[0;34m(bucket, s3_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mNoCredentialsError\u001b[49m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCredentials not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NoCredentialsError' is not defined"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(read_string_file_s3(bucket_name, file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90509df3-a638-4474-8cba-1ef138d10e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd7af9-6768-4c58-bebc-f5a6717c381c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3eae0-f695-4483-a4dc-951706782cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15de41d4-968a-4c47-b8e0-017db170cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9968a392-5a82-4431-9d20-f3f659af6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = google/flan-t5-large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f4f523c-13a0-4750-a672-8cb404af4b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7464bc2841459c881367fae58d2317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f797536d4f9f4ce1ad86287e27e01c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fc380433ea495a976e7a057c191828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e116a1d9a0ed4f8cbe5777db9a4d174a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59ca1f996074dda91d2bb1d6ebc9e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dbf1788ba447c0bdbd4260b14dd2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cd8fe6a9fb42d19b71f2172a61357a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline(summarization, model=model_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2abbacbb-6fa0-4043-9563-61fa0a7d4262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1256 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Little wonder the stock market has been bounding up and down like a yory'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\"THE ECONOMIC SCENE: The Recovery Still Lives. Little wonder the stock market has been bounding up and down like a yoryo lately. There are so many anomalies in the economic picture these days and so much uncertainty over what lies aheadr-for domestic business, inflation. Government Policies; ehersv ahd taxation as well as over the whole international pic* ture—that no one seems to have much reliance on aiiy forecasts, whatever they predict.</br></br>One week, or even a single day, produces reasons for cheer, but the very next period uncovers contrary data or assessments to chase the euphoria. Investors, like businessmen, don't know what to expect next.</br></br>Although the economy seldom moves in an unbroken trend in any direction, the gyrations this year seem to have been unusually pronounced. And so have the policy actions in .Washington. Even so, over time, the economy has managed to trace a steady and strongly upward course throughout the 33 months .of the current expansionary phase of the business cycle in this country. Moreover, the conseasus is that the continuing recovery still has considerable life left in it</br></br>Despite that fact, the stock market has been unwilling to look beyond the valley to the next crest. And businessmen have not been inclined to discount the question marks of the moment to gaze toward a better tomorrow. And yet, when all the pluses and minuses are tabulated, the favorable side of the ledger is definitely longer, and the bottom line shows ai positive balance.</br></br>The last few weeks provide a good example of the economy’s erratic fluctuations and the nervous state of current thinking in so many quarters.lt should have been a consoling period for analysts of the American business scene—with many more constructive.devel-opments than negative ones—but the sense of malaise continued to linger. Inventory of Unsold Homes Grows. A new factor is adding to (lie cost of buying a newly built home in this period of inflation, high interest rates cjnd a scarcity of mortgage money—the builder's delay in selling completed homes.</br></br>were 441,000 new homes without buyers in various stages of planning or construction, according to the Census Bureau. Of the total, first half of 1974, construction of new one-family homes fell 24 per cent from the comparable 1973 period.</br></br>“It looks as if the year’s total will fall below the million mark, the first time since 1970,” said a Census Bureau official. The million mark has been exceeded only four times in the nation’s history—in 1963, 1971, 1972 and 1973.</br></br>penses. In one form or another these are eventually passed on to the buyer. The expenses include maintain-ance, security, sales operation, taxes and carrying charges on the construction loan. “The builder may cut back his profit on a project, but someplace along the line those extra costs are going to be tacked on somewhere, maybe at another development,” said a spokesman for one major builder.</br></br>including gas. The Butman construction Corporation was the contractor and Ginsbern & Associates was the architect. James Felt-Huberth & Huberth are the managing and renting agents for the $3.4-million project. Thomas Eddy was i trustee of the bank in the early 19th Century and a philanthropist. Are Large Caps Back in Style?. ®M1D rising interest rates anti U market volatility, many In-vestment strategists say it is time to refocus on those core investments: large-capitalisation domestic stock funds, We feel very strongly that the tide has gone out with respect to more-risky asset classes,” said Nick Bolm-sack, investment strategist at the ISI Group in New York. Investors should look toward higher-quality asset classes, and in our view the most attractive is U.S. large caps.</br></br>Despite their usually prominent position in investor portfolios, large-capitalization domestic stocks and stock funds, which include Standard Despite the lackluster performance Of large-cap slocks, their earnings and price-to-earnings ratios have ' become more attractive.</br></br>& Poor’s 500-stock index funds, have been out of favor with investors in recent years, and no wonder. Domestic small-cap stock funds and emerging-market funds outperformed domestic large caps for the three years through April 2006, posting gains of better than 20 percent a year. The</br></br>Investors actually pulled a net $10.2 billion out of large-cap blend funds in 2005 and early 2006, according to the Financial Research Corporation. That was the first time investors took money out of this group of core stock funds since 1994, when the company began tracking fund categories. Large-cap blend funds meld growth and value characteristics in a single portfolio.</br></br>“Market history is highly cyclical: winners become losers and vice versa,” said Francis M. Kinniry Jr., a principal in the investment counseling and research group at Vanguard. “Investors would do themselves a huge service by going back to core funds,” he said.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c43e3-27d3-4eb2-a279-f883700ae748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db958bb2-3f0e-4200-ae34-895ed62f08f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e69a70-be0a-45a2-91f7-f4814bb25024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5345a1-bb32-4c4c-a224-201462d14f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7b284-523b-4a39-b915-e42827379c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
