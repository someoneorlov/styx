{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing legal documents with Hugging Face and Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of parameters for flan-t5 family: small 80M, base 250M, large 780M, xl 3B, xxl 11B\n",
    "model_id = \"google/flan-t5-xl\"\n",
    "\n",
    "# https://huggingface.co/datasets/abisee/cnn_dailymail\n",
    "dataset_name, dataset_version = \"cnn_dailymail\", \"3.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install widgetsnbextension ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "2.221.1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# bucket = sess.default_bucket()\n",
    "# bucket = \"styx-nlp-datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.2\n",
      "2.19.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_dataset(dataset_name, dataset_version)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502e6da99f154b21a9ab354554e5410d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4533f50b619a4a68a09823572a98107a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c542cd57699844bca42688e65e50eb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5dab475a544d17ba3a4c95d36d6b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "input_max_length = 2048\n",
    "output_max_length = 512\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=input_max_length, truncation=True)\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"highlights\"], max_length=output_max_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a433279ca04c9e985893e9829dfe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68e00d6aa2e4e59bfc496c13e60b32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efc7bd97be242fbb44ee02a3e3d24c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset.save_to_disk(f\"cnn_dailymail-t5-tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload processed dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://summary-model-data/huggingface/cnn_dailymail-t5-summarization\n",
      "s3://summary-model-data/huggingface/cnn_dailymail-t5-summarization/train\n",
      "s3://summary-model-data/huggingface/cnn_dailymail-t5-summarization/validation\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = \"summary-model-data\"\n",
    "s3_prefix = \"huggingface/cnn_dailymail-t5-summarization\"\n",
    "\n",
    "dataset_input_path = \"s3://{}/{}\".format(bucket, s3_prefix)\n",
    "train_input_path = \"{}/train\".format(dataset_input_path)\n",
    "valid_input_path = \"{}/validation\".format(dataset_input_path)\n",
    "\n",
    "print(dataset_input_path)\n",
    "print(train_input_path)\n",
    "print(valid_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74adff545ba14141b41fe9212ac7fc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b47da1069344d88c8682565efed40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/cnn_dailymail-t5-summarization/train/data-00000-of-00004.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/train/data-00001-of-00004.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/train/data-00002-of-00004.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/train/data-00003-of-00004.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/train/dataset_info.json\n",
      "huggingface/cnn_dailymail-t5-summarization/train/state.json\n",
      "huggingface/cnn_dailymail-t5-summarization/validation/data-00000-of-00001.arrow\n",
      "huggingface/cnn_dailymail-t5-summarization/validation/dataset_info.json\n",
      "huggingface/cnn_dailymail-t5-summarization/validation/state.json\n"
     ]
    }
   ],
   "source": [
    "# Save tokenized dataset locally first\n",
    "local_train_path = \"local_train_data\"\n",
    "local_valid_path = \"local_valid_data\"\n",
    "\n",
    "tokenized_dataset[\"train\"].save_to_disk(local_train_path)\n",
    "tokenized_dataset[\"test\"].save_to_disk(local_valid_path)\n",
    "\n",
    "# Upload the local files to S3\n",
    "for root, dirs, files in os.walk(local_train_path):\n",
    "    for file in files:\n",
    "        s3_client.upload_file(\n",
    "            os.path.join(root, file),\n",
    "            bucket,\n",
    "            os.path.join(s3_prefix, 'train', os.path.relpath(os.path.join(root, file), local_train_path))\n",
    "        )\n",
    "\n",
    "for root, dirs, files in os.walk(local_valid_path):\n",
    "    for file in files:\n",
    "        s3_client.upload_file(\n",
    "            os.path.join(root, file),\n",
    "            bucket,\n",
    "            os.path.join(s3_prefix, 'validation', os.path.relpath(os.path.join(root, file), local_valid_path))\n",
    "        )\n",
    "\n",
    "# Verify by listing the uploaded files\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=s3_prefix)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# s3 = S3FileSystem()\n",
    "\n",
    "# s3_prefix = \"huggingface/cnn_dailymail-t5-summarization\"\n",
    "\n",
    "# dataset_input_path = \"s3://{}/{}\".format(bucket, s3_prefix)\n",
    "# train_input_path = \"{}/train\".format(dataset_input_path)\n",
    "# valid_input_path = \"{}/validation\".format(dataset_input_path)\n",
    "\n",
    "# print(dataset_input_path)\n",
    "# print(train_input_path)\n",
    "# print(valid_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset[\"train\"].save_to_disk(train_input_path, fs=s3)\n",
    "# tokenized_dataset[\"test\"].save_to_disk(valid_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%sh -s $dataset_input_path\n",
    "#aws s3 ls --recursive $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = load_from_disk(train_input_path)\n",
    "#valid_ds = load_from_disk(valid_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune on SageMaker with a Hugging Face Deep Learning Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mevaluate\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    AutoModelForSeq2SeqLM,\u001b[37m\u001b[39;49;00m\n",
      "    AutoTokenizer,\u001b[37m\u001b[39;49;00m\n",
      "    DataCollatorForSeq2Seq,\u001b[37m\u001b[39;49;00m\n",
      "    Seq2SeqTrainer,\u001b[37m\u001b[39;49;00m\n",
      "    Seq2SeqTrainingArguments,\u001b[37m\u001b[39;49;00m\n",
      "    BitsAndBytesConfig\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpeft\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprint_trainable_parameters\u001b[39;49;00m(model):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Prints the number of trainable parameters in the model.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainable_params = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    all_param = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m _, param \u001b[35min\u001b[39;49;00m model.named_parameters():\u001b[37m\u001b[39;49;00m\n",
      "        all_param += param.numel()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m param.requires_grad:\u001b[37m\u001b[39;49;00m\n",
      "            trainable_params += param.numel()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtrainable params: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrainable_params\u001b[33m}\u001b[39;49;00m\u001b[33m || all params: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mall_param\u001b[33m}\u001b[39;49;00m\u001b[33m || trainable%: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[34m100\u001b[39;49;00m\u001b[37m \u001b[39;49;00m*\u001b[37m \u001b[39;49;00mtrainable_params\u001b[37m \u001b[39;49;00m/\u001b[37m \u001b[39;49;00mall_param\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(eval_pred):\u001b[37m\u001b[39;49;00m\n",
      "    predictions, labels = eval_pred\u001b[37m\u001b[39;49;00m\n",
      "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    labels = np.where(labels != -\u001b[34m100\u001b[39;49;00m, labels, tokenizer.pad_token_id)\u001b[37m\u001b[39;49;00m\n",
      "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) \u001b[34mfor\u001b[39;49;00m pred \u001b[35min\u001b[39;49;00m predictions]\u001b[37m\u001b[39;49;00m\n",
      "    result[\u001b[33m\"\u001b[39;49;00m\u001b[33mgen_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.mean(prediction_lens)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m {k: \u001b[36mround\u001b[39;49;00m(v, \u001b[34m4\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m result.items()}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# hyperparameters are passed as command-line arguments to the script.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m8\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--evaluation-strategy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save-strategy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mno\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save-steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--valid-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALID\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load metric\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    rouge = evaluate.load(\u001b[33m\"\u001b[39;49;00m\u001b[33mrouge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.train_dir)\u001b[37m\u001b[39;49;00m\n",
      "    valid_dataset = load_from_disk(args.valid_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtraining set: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_dataset\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation set: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalid_dataset\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# download and quantize model from model hub\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    bnb_config = BitsAndBytesConfig(\u001b[37m\u001b[39;49;00m\n",
      "        load_in_8bit=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name, quantization_config=bnb_config)\u001b[37m\u001b[39;49;00m\n",
      "    model.gradient_checkpointing_enable()\u001b[37m\u001b[39;49;00m\n",
      "    model = prepare_model_for_kbit_training(model)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    config = LoraConfig(\u001b[37m\u001b[39;49;00m\n",
      "        r=\u001b[34m16\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "        task_type=TaskType.SEQ_2_SEQ_LM,\u001b[37m\u001b[39;49;00m\n",
      "        inference_mode=\u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model = get_peft_model(model, config)\u001b[37m\u001b[39;49;00m\n",
      "    print_trainable_parameters(model)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# download the tokenizer too, which will be saved in the model artifact\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# and used at prediction time\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    training_args = Seq2SeqTrainingArguments(\u001b[37m\u001b[39;49;00m\n",
      "        output_dir=args.model_dir,\u001b[37m\u001b[39;49;00m\n",
      "        num_train_epochs=args.epochs,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_train_batch_size=args.train_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        save_strategy=args.save_strategy,\u001b[37m\u001b[39;49;00m\n",
      "        save_steps=args.save_steps,\u001b[37m\u001b[39;49;00m\n",
      "        evaluation_strategy=args.evaluation_strategy,\u001b[37m\u001b[39;49;00m\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\u001b[37m\u001b[39;49;00m\n",
      "        predict_with_generate=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        fp16=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# create trainer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer = Seq2SeqTrainer(\u001b[37m\u001b[39;49;00m\n",
      "        model=model,\u001b[37m\u001b[39;49;00m\n",
      "        args=training_args,\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "        train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        eval_dataset=valid_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        data_collator=data_collator,\u001b[37m\u001b[39;49;00m\n",
      "        compute_metrics=compute_metrics,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# train model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer.train()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 2,\n",
    "    \"learning-rate\": 1e-6,\n",
    "    \"train-batch-size\": 1,\n",
    "    \"eval-batch-size\": 8,\n",
    "    \"model-name\": model_id,\n",
    "    \"save-total-limit\": 3,\n",
    "    \"load-best-model-at-end\": True,\n",
    "    \"save-strategy\": \"epoch\",\n",
    "    \"evaluation-strategy\": \"epoch\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    # Fine-tuning script\n",
    "    entry_point=\"train.py\",\n",
    "    dependencies=[\"requirements.txt\"],\n",
    "    hyperparameters=hyperparameters,\n",
    "    # Infrastructure\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    instance_count=1,\n",
    "    use_spot_instances=True,\n",
    "    max_run=86400, # 24 hours\n",
    "    max_wait=86400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-06-01-21-15-02-130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-01 21:15:02 Starting - Starting the training job...\n",
      "2024-06-01 21:15:17 Starting - Preparing the instances for training...\n",
      "2024-06-01 21:15:46 Downloading - Downloading input data...\n",
      "2024-06-01 21:16:32 Downloading - Downloading the training image.................................\n",
      "2024-06-01 21:21:43 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-06-01 21:21:53,388 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-06-01 21:21:53,405 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-01 21:21:53,415 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-06-01 21:21:53,422 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-06-01 21:21:56,028 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers>4.30.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 3.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.4.0)\u001b[0m\n",
      "\u001b[34mCollecting rouge_score (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate>0.21 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>4.30.0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.23.0 (from transformers>4.30.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>4.30.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>4.30.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>4.30.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>4.30.0->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>4.30.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers>4.30.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1 (from transformers>4.30.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>4.30.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 4)) (2.1.0)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from rouge_score->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers>4.30.0->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>4.30.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>4.30.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>4.30.0->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>4.30.0->-r requirements.txt (line 1)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 5)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 5)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 5)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score->-r requirements.txt (line 4)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score->-r requirements.txt (line 4)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft->-r requirements.txt (line 5)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft->-r requirements.txt (line 5)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 98.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.11.1-py3-none-any.whl (251 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 31.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 20.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 30.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 39.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 60.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 65.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 72.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=fec1e60442265d58af09e6835f7c79ffa33fd3c0cec49aa9db2698e858149a34\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge_score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, nltk, rouge_score, huggingface-hub, tokenizers, bitsandbytes, accelerate, transformers, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.30.1 bitsandbytes-0.43.1 huggingface-hub-0.23.2 nltk-3.8.1 peft-0.11.1 rouge_score-0.1.2 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-06-01 21:22:13,245 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-01 21:22:13,245 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-01 21:22:13,282 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-01 21:22:13,311 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-01 21:22:13,339 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-01 21:22:13,349 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"valid\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2,\n",
      "        \"eval-batch-size\": 8,\n",
      "        \"evaluation-strategy\": \"epoch\",\n",
      "        \"learning-rate\": 1e-06,\n",
      "        \"load-best-model-at-end\": true,\n",
      "        \"model-name\": \"google/flan-t5-xl\",\n",
      "        \"save-strategy\": \"epoch\",\n",
      "        \"save-total-limit\": 3,\n",
      "        \"train-batch-size\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-06-01-21-15-02-130\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-381491949871/huggingface-pytorch-training-2024-06-01-21-15-02-130/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2,\"eval-batch-size\":8,\"evaluation-strategy\":\"epoch\",\"learning-rate\":1e-06,\"load-best-model-at-end\":true,\"model-name\":\"google/flan-t5-xl\",\"save-strategy\":\"epoch\",\"save-total-limit\":3,\"train-batch-size\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-381491949871/huggingface-pytorch-training-2024-06-01-21-15-02-130/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"eval-batch-size\":8,\"evaluation-strategy\":\"epoch\",\"learning-rate\":1e-06,\"load-best-model-at-end\":true,\"model-name\":\"google/flan-t5-xl\",\"save-strategy\":\"epoch\",\"save-total-limit\":3,\"train-batch-size\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2024-06-01-21-15-02-130\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-381491949871/huggingface-pytorch-training-2024-06-01-21-15-02-130/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\",\"--eval-batch-size\",\"8\",\"--evaluation-strategy\",\"epoch\",\"--learning-rate\",\"1e-06\",\"--load-best-model-at-end\",\"True\",\"--model-name\",\"google/flan-t5-xl\",\"--save-strategy\",\"epoch\",\"--save-total-limit\",\"3\",\"--train-batch-size\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL-BATCH-SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION-STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=1e-06\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD-BEST-MODEL-AT-END=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL-NAME=google/flan-t5-xl\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE-STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE-TOTAL-LIMIT=3\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN-BATCH-SIZE=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --epochs 2 --eval-batch-size 8 --evaluation-strategy epoch --learning-rate 1e-06 --load-best-model-at-end True --model-name google/flan-t5-xl --save-strategy epoch --save-total-limit 3 --train-batch-size 1\u001b[0m\n",
      "\u001b[34m2024-06-01 21:22:13,382 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 6.55MB/s]\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34m`low_cpu_mem_usage` was None, now set to True since model is quantized.\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:56<00:56, 56.60s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [01:11<00:00, 31.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [01:11<00:00, 35.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [01:15<01:15, 75.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [01:29<00:00, 39.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [01:29<00:00, 44.89s/it]\u001b[0m\n",
      "\u001b[34mtrainable params: 9437184 || all params: 2859194368 || trainable%: 0.33006444422319176\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/574226 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m0%|          | 1/574226 [00:02<363:13:44,  2.28s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/574226 [00:03<294:08:25,  1.84s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/574226 [00:04<239:42:09,  1.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/574226 [00:06<225:04:02,  1.41s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/574226 [00:09<314:21:34,  1.97s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/574226 [00:10<282:23:49,  1.77s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/574226 [00:11<257:12:13,  1.61s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/574226 [00:12<229:14:20,  1.44s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 9/574226 [00:13<205:50:45,  1.29s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 10/574226 [00:16<289:23:49,  1.81s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 11/574226 [00:18<266:38:10,  1.67s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 12/574226 [00:19<236:50:40,  1.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 13/574226 [00:20<221:05:53,  1.39s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 14/574226 [00:22<265:14:09,  1.66s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 15/574226 [00:24<263:49:34,  1.65s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 16/574226 [00:25<251:47:44,  1.58s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 17/574226 [00:27<239:09:14,  1.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 18/574226 [00:28<255:12:06,  1.60s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 19/574226 [00:31<297:15:47,  1.86s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 20/574226 [00:33<298:05:10,  1.87s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 21/574226 [00:34<266:17:19,  1.67s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 22/574226 [00:35<244:35:01,  1.53s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 23/574226 [00:37<249:23:43,  1.56s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 24/574226 [00:39<285:54:38,  1.79s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 25/574226 [00:42<331:48:33,  2.08s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 26/574226 [00:43<305:54:06,  1.92s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 27/574226 [00:45<289:05:16,  1.81s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 28/574226 [00:47<280:38:04,  1.76s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 29/574226 [00:48<252:43:36,  1.58s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 30/574226 [00:49<257:18:27,  1.61s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 31/574226 [00:51<245:58:14,  1.54s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 32/574226 [00:52<234:57:38,  1.47s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 33/574226 [00:54<230:18:41,  1.44s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 34/574226 [00:55<216:52:12,  1.36s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 35/574226 [00:56<206:48:13,  1.30s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 36/574226 [00:57<208:24:04,  1.31s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 37/574226 [00:59<212:23:05,  1.33s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 38/574226 [01:00<238:22:17,  1.49s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 39/574226 [01:01<216:52:33,  1.36s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 40/574226 [01:03<202:48:59,  1.27s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 41/574226 [01:04<198:36:38,  1.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 42/574226 [01:06<258:18:38,  1.62s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 43/574226 [01:07<224:14:28,  1.41s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 44/574226 [01:10<280:49:29,  1.76s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 45/574226 [01:11<248:30:30,  1.56s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 46/574226 [01:12<222:59:32,  1.40s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 47/574226 [01:13<232:48:04,  1.46s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 48/574226 [01:16<306:57:38,  1.92s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 49/574226 [01:18<279:06:41,  1.75s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 50/574226 [01:19<258:23:05,  1.62s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 51/574226 [01:22<323:35:31,  2.03s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 52/574226 [01:24<299:53:39,  1.88s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 53/574226 [01:27<352:51:02,  2.21s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 54/574226 [01:28<307:48:22,  1.93s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 55/574226 [01:29<262:25:30,  1.65s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 56/574226 [01:30<232:55:43,  1.46s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 57/574226 [01:31<214:05:28,  1.34s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 58/574226 [01:32<215:11:47,  1.35s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 59/574226 [01:35<264:12:31,  1.66s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 60/574226 [01:37<282:46:53,  1.77s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 61/574226 [01:38<255:07:12,  1.60s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 62/574226 [01:40<261:59:40,  1.64s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 63/574226 [01:41<251:53:59,  1.58s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 64/574226 [01:43<283:29:36,  1.78s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 65/574226 [01:46<315:51:34,  1.98s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 66/574226 [01:47<283:57:08,  1.78s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 67/574226 [01:50<320:20:10,  2.01s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 68/574226 [01:51<292:25:02,  1.83s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 69/574226 [01:53<280:37:58,  1.76s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 70/574226 [01:54<254:34:44,  1.60s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 71/574226 [01:55<240:08:02,  1.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 72/574226 [01:56<220:08:43,  1.38s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 73/574226 [01:57<209:33:48,  1.31s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 74/574226 [02:00<264:42:24,  1.66s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 75/574226 [02:02<280:47:52,  1.76s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 76/574226 [02:03<245:07:16,  1.54s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 77/574226 [02:05<249:59:10,  1.57s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 78/574226 [02:06<235:15:53,  1.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 79/574226 [02:07<236:47:12,  1.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 80/574226 [02:08<221:42:48,  1.39s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 81/574226 [02:10<224:13:37,  1.41s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 82/574226 [02:12<232:43:05,  1.46s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 83/574226 [02:13<219:31:36,  1.38s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 84/574226 [02:14<225:22:57,  1.41s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 85/574226 [02:17<301:04:25,  1.89s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 86/574226 [02:20<346:21:07,  2.17s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 87/574226 [02:21<307:56:07,  1.93s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 88/574226 [02:23<271:18:11,  1.70s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 89/574226 [02:24<251:02:48,  1.57s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 90/574226 [02:25<242:09:33,  1.52s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 91/574226 [02:26<230:18:20,  1.44s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 92/574226 [02:28<238:10:59,  1.49s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 93/574226 [02:30<265:21:27,  1.66s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 94/574226 [02:31<235:29:48,  1.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 95/574226 [02:32<215:39:29,  1.35s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 96/574226 [02:34<216:06:09,  1.36s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 97/574226 [02:35<194:42:34,  1.22s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 98/574226 [02:36<194:22:46,  1.22s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 99/574226 [02:37<198:13:55,  1.24s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 100/574226 [02:38<201:20:48,  1.26s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 101/574226 [02:40<206:46:04,  1.30s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 102/574226 [02:41<188:56:17,  1.18s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 103/574226 [02:42<180:52:27,  1.13s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 104/574226 [02:44<222:55:07,  1.40s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 105/574226 [02:45<213:12:07,  1.34s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 106/574226 [02:47<250:03:26,  1.57s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 107/574226 [02:48<221:07:01,  1.39s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 108/574226 [02:49<207:26:29,  1.30s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 109/574226 [02:50<195:14:55,  1.22s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 110/574226 [02:51<193:56:09,  1.22s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 111/574226 [02:53<197:14:12,  1.24s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 112/574226 [02:55<270:06:31,  1.69s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 113/574226 [02:58<305:42:32,  1.92s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 114/574226 [02:59<279:41:19,  1.75s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 115/574226 [03:02<339:47:33,  2.13s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 116/574226 [03:03<297:01:36,  1.86s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 117/574226 [03:05<284:18:34,  1.78s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 118/574226 [03:06<257:11:59,  1.61s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 119/574226 [03:07<241:18:04,  1.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 120/574226 [03:09<229:48:06,  1.44s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 121/574226 [03:10<214:15:31,  1.34s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 122/574226 [03:11<199:20:58,  1.25s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 123/574226 [03:13<215:57:30,  1.35s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 124/574226 [03:14<223:42:20,  1.40s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 125/574226 [03:16<239:00:22,  1.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 126/574226 [03:18<276:27:40,  1.73s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 127/574226 [03:19<244:40:02,  1.53s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 128/574226 [03:20<227:57:06,  1.43s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 129/574226 [03:22<252:44:52,  1.58s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 130/574226 [03:24<277:40:58,  1.74s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 131/574226 [03:26<261:16:45,  1.64s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 132/574226 [03:27<260:20:00,  1.63s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 133/574226 [03:29<251:41:46,  1.58s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 134/574226 [03:32<309:22:24,  1.94s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 135/574226 [03:34<316:13:22,  1.98s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 136/574226 [03:37<361:31:57,  2.27s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 137/574226 [03:39<369:42:07,  2.32s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 138/574226 [03:41<334:54:41,  2.10s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 139/574226 [03:42<291:47:08,  1.83s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 140/574226 [03:44<309:18:06,  1.94s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 141/574226 [03:45<267:57:23,  1.68s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 142/574226 [03:47<281:13:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 143/574226 [03:48<241:59:21,  1.52s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 144/574226 [03:49<225:10:23,  1.41s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 145/574226 [03:51<258:41:20,  1.62s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 146/574226 [03:53<239:30:17,  1.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 147/574226 [03:56<311:15:02,  1.95s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 148/574226 [03:56<262:50:25,  1.65s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 149/574226 [03:58<241:09:13,  1.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 150/574226 [03:59<233:53:53,  1.47s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 151/574226 [04:00<211:27:10,  1.33s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 152/574226 [04:01<193:27:53,  1.21s/it]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_input_path\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1346\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2703\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2703\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:5797\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5778\u001b[0m \n\u001b[1;32m   5779\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5795\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5796\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5797\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:7976\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE:\n\u001b[1;32m   7974\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 7976\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   7979\u001b[0m     state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({\"train\": train_input_path, \"valid\": valid_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_id = \"juliensimon/flan-t5-large-billsum-qlora\"\n",
    "base_model_id = \"google/flan-t5-large\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_id)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_id)\n",
    "model = PeftModel.from_pretrained(base_model, model_id)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"billsum\")\n",
    "sample = dataset['test'][123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(sample[\"text\"], return_tensors=\"pt\", truncation=True).input_ids\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=64, do_sample=True, top_p=0.8)\n",
    "\n",
    "tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
